{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow BYOM: Train with Custom Training Script, Compile with Neo, and Deploy on SageMaker\n",
    "\n",
    "This notebook can be compared to [TensorFlow MNIST distributed training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_distributed_mnist.ipynb) in terms of its functionality. We will do the same classification task, but this time we will compile the trained model using the Neo API backend, to optimize for our choice of hardware. Finally, we setup a real-time hosted endpoint in SageMaker for our compiled model using the Neo Deep Learning Runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add note on pink warnings!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Writing data/train.tfrecords\n",
      "Writing data/validation.tfrecords\n",
      "Writing data/test.tfrecords\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "data_sets = mnist.read_data_sets('data', dtype=tf.uint8, reshape=False, validation_size=5000)\n",
    "\n",
    "utils.convert_to(data_sets.train, 'train', 'data')\n",
    "utils.convert_to(data_sets.validation, 'validation', 'data')\n",
    "utils.convert_to(data_sets.test, 'test', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data\n",
    "We use the ```sagemaker.Session.upload_data``` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for distributed training \n",
    "Here is the full code for the network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.python.estimator.model_fn\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ModeKeys \u001b[34mas\u001b[39;49;00m Modes\r\n",
      "\r\n",
      "INPUT_TENSOR_NAME = \u001b[33m'\u001b[39;49;00m\u001b[33minputs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "SIGNATURE_NAME = \u001b[33m'\u001b[39;49;00m\u001b[33mpredictions\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\r\n",
      "LEARNING_RATE = \u001b[34m0.001\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(features, labels, mode, params):\r\n",
      "    \u001b[37m# Input Layer\u001b[39;49;00m\r\n",
      "    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-\u001b[34m1\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[37m# Convolutional Layer #1\u001b[39;49;00m\r\n",
      "    conv1 = tf.layers.conv2d(\r\n",
      "        inputs=input_layer,\r\n",
      "        filters=\u001b[34m32\u001b[39;49;00m,\r\n",
      "        kernel_size=[\u001b[34m5\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m],\r\n",
      "        padding=\u001b[33m'\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "        activation=tf.nn.relu)\r\n",
      "\r\n",
      "    \u001b[37m# Pooling Layer #1\u001b[39;49;00m\r\n",
      "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[\u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m], strides=\u001b[34m2\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Convolutional Layer #2 and Pooling Layer #2\u001b[39;49;00m\r\n",
      "    conv2 = tf.layers.conv2d(\r\n",
      "        inputs=pool1,\r\n",
      "        filters=\u001b[34m64\u001b[39;49;00m,\r\n",
      "        kernel_size=[\u001b[34m5\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m],\r\n",
      "        padding=\u001b[33m'\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "        activation=tf.nn.relu)\r\n",
      "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[\u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m], strides=\u001b[34m2\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Dense Layer\u001b[39;49;00m\r\n",
      "    pool2_flat = tf.reshape(pool2, [-\u001b[34m1\u001b[39;49;00m, \u001b[34m7\u001b[39;49;00m * \u001b[34m7\u001b[39;49;00m * \u001b[34m64\u001b[39;49;00m])\r\n",
      "    dense = tf.layers.dense(inputs=pool2_flat, units=\u001b[34m1024\u001b[39;49;00m, activation=tf.nn.relu)\r\n",
      "    dropout = tf.layers.dropout(\r\n",
      "        inputs=dense, rate=\u001b[34m0.4\u001b[39;49;00m, training=(mode == Modes.TRAIN))\r\n",
      "\r\n",
      "    \u001b[37m# Logits Layer\u001b[39;49;00m\r\n",
      "    logits = tf.layers.dense(inputs=dropout, units=\u001b[34m10\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Define operations\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m mode \u001b[35min\u001b[39;49;00m (Modes.PREDICT, Modes.EVAL):\r\n",
      "        predicted_indices = tf.argmax(\u001b[36minput\u001b[39;49;00m=logits, axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "        probabilities = tf.nn.softmax(logits, name=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax_tensor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m mode \u001b[35min\u001b[39;49;00m (Modes.TRAIN, Modes.EVAL):\r\n",
      "        global_step = tf.train.get_or_create_global_step()\r\n",
      "        label_indices = tf.cast(labels, tf.int32)\r\n",
      "        loss = tf.losses.softmax_cross_entropy(\r\n",
      "            onehot_labels=tf.one_hot(label_indices, depth=\u001b[34m10\u001b[39;49;00m), logits=logits)\r\n",
      "        tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mOptimizeLoss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, loss)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m mode == Modes.PREDICT:\r\n",
      "        predictions = {\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mclasses\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: predicted_indices,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mprobabilities\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: probabilities\r\n",
      "        }\r\n",
      "        export_outputs = {\r\n",
      "            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\r\n",
      "        }\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m tf.estimator.EstimatorSpec(\r\n",
      "            mode, predictions=predictions, export_outputs=export_outputs)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m mode == Modes.TRAIN:\r\n",
      "        optimizer = tf.train.AdamOptimizer(learning_rate=\u001b[34m0.001\u001b[39;49;00m)\r\n",
      "        train_op = optimizer.minimize(loss, global_step=global_step)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m mode == Modes.EVAL:\r\n",
      "        eval_metric_ops = {\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.metrics.accuracy(label_indices, predicted_indices)\r\n",
      "        }\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m tf.estimator.EstimatorSpec(\r\n",
      "            mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mserving_input_fn\u001b[39;49;00m(params):\r\n",
      "    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [\u001b[36mNone\u001b[39;49;00m, \u001b[34m784\u001b[39;49;00m])}\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m tf.estimator.export.ServingInputReceiver(inputs, inputs)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mread_and_decode\u001b[39;49;00m(filename_queue):\r\n",
      "    reader = tf.TFRecordReader()\r\n",
      "    _, serialized_example = reader.read(filename_queue)\r\n",
      "\r\n",
      "    features = tf.parse_single_example(\r\n",
      "        serialized_example,\r\n",
      "        features={\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mimage_raw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.FixedLenFeature([], tf.string),\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.FixedLenFeature([], tf.int64),\r\n",
      "        })\r\n",
      "\r\n",
      "    image = tf.decode_raw(features[\u001b[33m'\u001b[39;49;00m\u001b[33mimage_raw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], tf.uint8)\r\n",
      "    image.set_shape([\u001b[34m784\u001b[39;49;00m])\r\n",
      "    image = tf.cast(image, tf.float32) * (\u001b[34m1.\u001b[39;49;00m / \u001b[34m255\u001b[39;49;00m)\r\n",
      "    label = tf.cast(features[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], tf.int32)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m image, label\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_input_fn\u001b[39;49;00m(training_dir, params):\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m _input_fn(training_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.tfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, batch_size=\u001b[34m100\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32meval_input_fn\u001b[39;49;00m(training_dir, params):\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m _input_fn(training_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtest.tfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, batch_size=\u001b[34m100\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_input_fn\u001b[39;49;00m(training_dir, training_filename, batch_size=\u001b[34m100\u001b[39;49;00m):\r\n",
      "    test_file = os.path.join(training_dir, training_filename)\r\n",
      "    filename_queue = tf.train.string_input_producer([test_file])\r\n",
      "\r\n",
      "    image, label = read_and_decode(filename_queue)\r\n",
      "    images, labels = tf.train.batch(\r\n",
      "        [image, label], batch_size=batch_size,\r\n",
      "        capacity=\u001b[34m1000\u001b[39;49;00m + \u001b[34m3\u001b[39;49;00m * batch_size)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m {INPUT_TENSOR_NAME: images}, labels\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mneo_preprocess\u001b[39;49;00m(payload, content_type):\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\r\n",
      "\r\n",
      "    logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mInvoking user-defined pre-processing function\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m content_type != \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-image\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m content_type != \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/vnd+python.numpy+binary\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mContent type must be application/x-image or application/vnd+python.numpy+binary\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    f = io.BytesIO(payload)\r\n",
      "    image = np.load(f)*\u001b[34m255\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m image\r\n",
      "\r\n",
      "\u001b[37m### NOTE: this function cannot use MXNet\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mneo_postprocess\u001b[39;49;00m(result):\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\r\n",
      "    logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mInvoking user-defined post-processing function\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# Softmax (assumes batch size 1)\u001b[39;49;00m\r\n",
      "    result = np.squeeze(result)\r\n",
      "    result_exp = np.exp(result - np.max(result))\r\n",
      "    result = result_exp / np.sum(result_exp)\r\n",
      "\r\n",
      "    response_body = json.dumps(result.tolist())\r\n",
      "    content_type = \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m response_body, content_type\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script here is and adaptation of the [TensorFlow MNIST example](https://github.com/tensorflow/models/tree/master/official/mnist). It provides a ```model_fn(features, labels, mode)```, which is used for training, evaluation and inference. See [TensorFlow MNIST distributed training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_distributed_mnist.ipynb) for more details about the training script.\n",
    "\n",
    "At the end of the training script, there are two additional functions, to be used with Neo Deep Learning Runtime:\n",
    "* `neo_preprocess(payload, content_type)`: Function that takes in the payload and Content-Type of each incoming request and returns a NumPy array\n",
    "* `neo_postprocess(result)`: Function that takes the prediction results produced by Deep Learining Runtime and returns the response body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.TensorFlow estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1017 23:21:27.011585 140475592632128 estimator.py:293] tensorflow py2 container will be deprecated soon.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-17 23:21:27 Starting - Starting the training job...\n",
      "2019-10-17 23:21:29 Starting - Launching requested ML instances......\n",
      "2019-10-17 23:22:35 Starting - Preparing the instances for training......\n",
      "2019-10-17 23:23:52 Downloading - Downloading input data\n",
      "2019-10-17 23:23:52 Training - Downloading the training image..\u001b[32m2019-10-17 23:24:04,875 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:04,875 INFO - root - starting train task\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:04,888 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[32mDownloading s3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:07,588 INFO - tf_container - ----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:07,589 INFO - tf_container - {\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"algo-2:2222\"], \"ps\": [\"algo-1:2223\", \"algo-2:2223\"], \"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:07,589 INFO - tf_container - ---------------------------------------------------------\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:07,589 INFO - tf_container - creating RunConfig:\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:07,589 INFO - tf_container - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:07,589 INFO - tensorflow - TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'algo-1:2223', u'algo-2:2223'], u'worker': [u'algo-2:2222'], u'master': [u'algo-1:2222']}, u'task': {u'index': 0, u'type': u'worker'}}\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:07,590 INFO - tf_container - creating an estimator from the user-provided model_fn\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:07,590 INFO - tensorflow - Using config: {'_save_checkpoints_secs': 300, '_keep_checkpoint_max': 5, '_task_type': u'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9999f4ba10>, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 2, '_tf_random_seed': None, '_device_fn': None, '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_session_config': device_filters: \"/job:ps\"\u001b[0m\n",
      "\u001b[32mdevice_filters: \"/job:worker/task:0\"\u001b[0m\n",
      "\u001b[32mallow_soft_placement: true\u001b[0m\n",
      "\u001b[32mgraph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32m, '_global_id_in_cluster': 1, '_is_chief': False, '_protocol': None, '_save_checkpoints_steps': None, '_experimental_distribute': None, '_save_summary_steps': 100, '_model_dir': u's3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/checkpoints', '_master': u'grpc://algo-2:2222'}\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:07,592 INFO - tensorflow - Not using Distribute Coordinator.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:07,593 INFO - tensorflow - Start Tensorflow server.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:07,627 INFO - tensorflow - Waiting 5 secs before starting training.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:05,594 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:05,595 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:05,608 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,387 INFO - tf_container - ----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,387 INFO - tf_container - {\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"algo-2:2222\"], \"ps\": [\"algo-1:2223\", \"algo-2:2223\"], \"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,387 INFO - tf_container - ---------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,388 INFO - tf_container - creating RunConfig:\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,388 INFO - tf_container - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,388 INFO - tensorflow - TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'algo-1:2223', u'algo-2:2223'], u'worker': [u'algo-2:2222'], u'master': [u'algo-1:2222']}, u'task': {u'index': 0, u'type': u'master'}}\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,389 INFO - tf_container - creating an estimator from the user-provided model_fn\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,390 INFO - tensorflow - Using config: {'_save_checkpoints_secs': 300, '_keep_checkpoint_max': 5, '_task_type': u'master', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1db3fb1a10>, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 2, '_tf_random_seed': None, '_device_fn': None, '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_session_config': device_filters: \"/job:ps\"\u001b[0m\n",
      "\u001b[31mdevice_filters: \"/job:master\"\u001b[0m\n",
      "\u001b[31mallow_soft_placement: true\u001b[0m\n",
      "\u001b[31mgraph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m, '_global_id_in_cluster': 0, '_is_chief': True, '_protocol': None, '_save_checkpoints_steps': None, '_experimental_distribute': None, '_save_summary_steps': 100, '_model_dir': u's3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/checkpoints', '_master': u'grpc://algo-1:2222'}\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,392 INFO - tensorflow - Not using Distribute Coordinator.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,393 INFO - tensorflow - Start Tensorflow server.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,669 WARNING - tensorflow - From /opt/ml/code/mnist.py:114: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,677 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,679 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,681 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:197: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,683 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,687 WARNING - tensorflow - From /opt/ml/code/mnist.py:86: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,698 WARNING - tensorflow - From /opt/ml/code/mnist.py:119: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:08,707 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:09,027 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:09,028 INFO - tensorflow - Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:09,512 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:09,875 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:09,881 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:09,914 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:10,496 INFO - tensorflow - Saving checkpoints for 0 into s3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/checkpoints/model.ckpt.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-10-17 23:24:05 Training - Training image download completed. Training in progress.\u001b[32m2019-10-17 23:24:13,791 WARNING - tensorflow - From /opt/ml/code/mnist.py:114: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:13,799 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:13,801 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:13,803 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:197: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:13,805 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:13,810 WARNING - tensorflow - From /opt/ml/code/mnist.py:86: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:13,818 WARNING - tensorflow - From /opt/ml/code/mnist.py:119: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:13,828 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:14,132 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:14,134 INFO - tensorflow - Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:14,632 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:14,704 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:14,710 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:14,740 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:15,239 INFO - tensorflow - loss = 2.3060033, step = 0\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:15,386 INFO - tensorflow - loss = 2.0831416, step = 0\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:22,717 INFO - tensorflow - global_step/sec: 14.0898\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:28,031 INFO - tensorflow - loss = 0.0879329, step = 184 (12.645 sec)\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:29,380 INFO - tensorflow - global_step/sec: 15.1586\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:30,577 INFO - tensorflow - loss = 0.032767687, step = 220 (15.338 sec)\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:36,310 INFO - tensorflow - global_step/sec: 14.5746\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:40,680 INFO - tensorflow - loss = 0.063772745, step = 370 (12.649 sec)\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:43,331 INFO - tensorflow - global_step/sec: 14.5285\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:45,061 INFO - tensorflow - loss = 0.051705103, step = 433 (14.483 sec)\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:50,241 INFO - tensorflow - global_step/sec: 14.7602\u001b[0m\n",
      "\u001b[31m2019-10-17 23:24:53,362 INFO - tensorflow - loss = 0.037559543, step = 556 (12.682 sec)\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:57,094 INFO - tensorflow - global_step/sec: 14.7394\u001b[0m\n",
      "\u001b[32m2019-10-17 23:24:59,750 INFO - tensorflow - loss = 0.06276701, step = 650 (14.689 sec)\u001b[0m\n",
      "\u001b[32m2019-10-17 23:25:04,153 INFO - tensorflow - global_step/sec: 14.5905\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:06,079 INFO - tensorflow - loss = 0.04417968, step = 742 (12.717 sec)\u001b[0m\n",
      "\u001b[32m2019-10-17 23:25:11,176 INFO - tensorflow - global_step/sec: 14.6653\u001b[0m\n",
      "\u001b[32m2019-10-17 23:25:14,550 INFO - tensorflow - loss = 0.019188566, step = 866 (14.799 sec)\u001b[0m\n",
      "\u001b[32m2019-10-17 23:25:18,152 INFO - tensorflow - global_step/sec: 14.6233\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:18,699 INFO - tensorflow - loss = 0.006484201, step = 927 (12.620 sec)\u001b[0m\n",
      "\u001b[32m2019-10-17 23:25:23,830 INFO - tensorflow - Loss for final step: 0.003410859.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:23,584 INFO - tensorflow - Saving checkpoints for 1001 into s3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:26,027 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:26,138 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:26,159 INFO - tensorflow - Starting evaluation at 2019-10-17-23:25:26\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:26,233 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:26,290 INFO - tensorflow - Restoring parameters from s3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:26,663 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:26,672 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:27,246 INFO - tensorflow - Evaluation [10/100]\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:27,722 INFO - tensorflow - Evaluation [20/100]\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:28,144 INFO - tensorflow - Evaluation [30/100]\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:28,640 INFO - tensorflow - Evaluation [40/100]\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:28,995 INFO - tensorflow - Evaluation [50/100]\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:29,353 INFO - tensorflow - Evaluation [60/100]\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:29,814 INFO - tensorflow - Evaluation [70/100]\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:30,283 INFO - tensorflow - Evaluation [80/100]\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:30,748 INFO - tensorflow - Evaluation [90/100]\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:31,218 INFO - tensorflow - Evaluation [100/100]\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:31,246 INFO - tensorflow - Finished evaluation at 2019-10-17-23:25:31\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:31,247 INFO - tensorflow - Saving dict for global step 1002: accuracy = 0.9906, global_step = 1002, loss = 0.02782892\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:31,732 INFO - tensorflow - Saving 'checkpoint_path' summary for global step 1002: s3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:32,151 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:32,211 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:32,211 INFO - tensorflow - Signatures INCLUDED in export for Eval: None\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:32,211 INFO - tensorflow - Signatures INCLUDED in export for Classify: None\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:32,212 INFO - tensorflow - Signatures INCLUDED in export for Regress: None\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:32,212 INFO - tensorflow - Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:32,212 INFO - tensorflow - Signatures INCLUDED in export for Train: None\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:32,291 INFO - tensorflow - Restoring parameters from s3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:32,594 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py:1046: calling add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mPass your op to the equivalent parameter main_op instead.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:32,594 INFO - tensorflow - Assets added to graph.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:32,594 INFO - tensorflow - No assets to write.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m2019-10-17 23:25:34,329 INFO - tensorflow - SavedModel written to: s3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/checkpoints/export/Servo/1571354731/saved_model.pb\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:34,443 INFO - tensorflow - Loss for final step: 0.001797853.\u001b[0m\n",
      "\u001b[31m2019-10-17 23:25:34,747 INFO - tf_container - Downloaded saved model at /opt/ml/model/export/Servo/1571354731\u001b[0m\n",
      "\n",
      "2019-10-17 23:27:00 Uploading - Uploading generated training model\n",
      "2019-10-17 23:27:00 Completed - Training job completed\n",
      "\u001b[32m2019-10-17 23:26:51,460 INFO - tf_container - master algo-1 is down, stopping parameter server\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             framework_version='1.12.0',\n",
    "                             training_steps=1000, \n",
    "                             evaluation_steps=100,\n",
    "                             train_instance_count=2,\n",
    "                             train_instance_type='ml.c4.xlarge',\n",
    "                             base_job_name='neo-compile-test')\n",
    "\n",
    "mnist_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **```fit```** method will create a training job in two **ml.c4.xlarge** instances. The logs above will show the instances doing training, evaluation, and incrementing the number of **training steps**. \n",
    "\n",
    "In the end of the training, the training job will generate a saved model for TF serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deploy the trained model to prepare for predictions (the old way)\n",
    "\n",
    "The deploy() method creates an endpoint which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1018 03:04:40.925576 140475592632128 model.py:103] The Python 2 tensorflow images will be soon deprecated and may not be supported for newer upcoming versions of the tensorflow images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 tensorflow image.\n",
      "W1018 03:04:41.245883 140475592632128 session.py:783] Using already existing model: neo-compile-test-2019-10-17-23-21-27-014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "mnist_predictor = mnist_estimator.deploy(initial_instance_count=1,\n",
    "                                         instance_type='ml.m4.xlarge',endpoint_name='original-mnist-model-endpoint-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Deploy the trained model using Neo\n",
    "\n",
    "Now the model is ready to be compiled by Neo to be optimized for our hardware of choice. We are using the  ``TensorFlowEstimator.compile_model`` method to do this. For this example, our target hardware is ``'ml_m4'``. You can changed these to other supported target hardware if you prefer.\n",
    "\n",
    "## Compiling the model\n",
    "The ``input_shape`` is the definition for the model's input tensor and ``output_path`` is where the compiled model will be stored in S3. **Important. If the following command result in a permission error, scroll up and locate the value of execution role returned by `get_execution_role()`. The role must have access to the S3 bucket specified in ``output_path``.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile for an EC2 target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1018 03:11:55.960345 140475592632128 model.py:103] The Python 2 tensorflow images will be soon deprecated and may not be supported for newer upcoming versions of the tensorflow images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 tensorflow image.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?.....!"
     ]
    }
   ],
   "source": [
    "target = 'ml_m4'\n",
    "output_path = mnist_estimator.output_path + target\n",
    "optimized_estimator_ml_m4 = mnist_estimator.compile_model(target_instance_family=target, \n",
    "                              input_shape={'data':[1, 784]},  # Batch size 1, 3 channels, 224x224 Images.\n",
    "                              output_path=output_path,\n",
    "                              framework='tensorflow', framework_version='1.12.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile for NVIDIA Jetson nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1018 03:12:27.240407 140475592632128 model.py:103] The Python 2 tensorflow images will be soon deprecated and may not be supported for newer upcoming versions of the tensorflow images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 tensorflow image.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?....!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1018 03:12:53.606170 140475592632128 model.py:369] The instance type jetson_nano is not supported to deploy via SageMaker,please deploy the model manually.\n"
     ]
    }
   ],
   "source": [
    "target = 'jetson_nano'\n",
    "output_path = mnist_estimator.output_path + target\n",
    "optimized_estimator_nano = mnist_estimator.compile_model(target_instance_family=target, \n",
    "                              input_shape={'data':[1, 784]},  # Batch size 1, 3 channels, 224x224 Images.\n",
    "                              output_path=output_path,\n",
    "                              framework='tensorflow', framework_version='1.12.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile for Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1018 03:12:53.641491 140475592632128 model.py:103] The Python 2 tensorflow images will be soon deprecated and may not be supported for newer upcoming versions of the tensorflow images.\n",
      "Please set the argument \"py_version='py3'\" to use the Python 3 tensorflow image.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?....!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1018 03:13:19.807258 140475592632128 model.py:369] The instance type rasp3b is not supported to deploy via SageMaker,please deploy the model manually.\n"
     ]
    }
   ],
   "source": [
    "target = 'rasp3b'\n",
    "output_path = mnist_estimator.output_path + target\n",
    "optimized_estimator_rpi = mnist_estimator.compile_model(target_instance_family=target, \n",
    "                              input_shape={'data':[1, 784]},  # Batch size 1, 3 channels, 224x224 Images.\n",
    "                              output_path=output_path,\n",
    "                              framework='tensorflow', framework_version='1.12.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiled model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(estimator):\n",
    "    out= !aws s3 ls {estimator.model_data} --human-readable\n",
    "    return out[0].split(' ')[-3]+' MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Targets</th>\n",
       "      <th>Locations</th>\n",
       "      <th>Sizes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original</td>\n",
       "      <td>s3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/output/model.tar.gz</td>\n",
       "      <td>11.6 MB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EC2 M4</td>\n",
       "      <td>s3://sagemaker-us-east-1-497456752804/ml_m4/model-ml_m4.tar.gz</td>\n",
       "      <td>11.6 MB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Raspberry Pi</td>\n",
       "      <td>s3://sagemaker-us-east-1-497456752804/rasp3b/model-rasp3b.tar.gz</td>\n",
       "      <td>11.6 MB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jetson Nano</td>\n",
       "      <td>s3://sagemaker-us-east-1-497456752804/jetson_nano/model-jetson_nano.tar.gz</td>\n",
       "      <td>11.6 MB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Targets  \\\n",
       "0  Original       \n",
       "1  EC2 M4         \n",
       "2  Raspberry Pi   \n",
       "3  Jetson Nano    \n",
       "\n",
       "                                                                                            Locations  \\\n",
       "0  s3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/output/model.tar.gz   \n",
       "1  s3://sagemaker-us-east-1-497456752804/ml_m4/model-ml_m4.tar.gz                                       \n",
       "2  s3://sagemaker-us-east-1-497456752804/rasp3b/model-rasp3b.tar.gz                                     \n",
       "3  s3://sagemaker-us-east-1-497456752804/jetson_nano/model-jetson_nano.tar.gz                           \n",
       "\n",
       "     Sizes  \n",
       "0  11.6 MB  \n",
       "1  11.6 MB  \n",
       "2  11.6 MB  \n",
       "3  11.6 MB  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "estimators = [mnist_estimator, optimized_estimator_ml_m4, optimized_estimator_rpi, optimized_estimator_nano] \n",
    "targets = ['Original','EC2 M4','Raspberry Pi','Jetson Nano']\n",
    "locations = [e.model_data for e in estimators]\n",
    "sizes = [get_model_size(e) for e in estimators]\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.DataFrame(list(zip(targets,locations,sizes)), columns =['Targets', 'Locations','Sizes']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "optimized_predictor = optimized_estimator_ml_m4.deploy(initial_instance_count = 1,\n",
    "                                                 instance_type = 'ml.m4.xlarge', endpoint_name='compiled-m4-mnist-model-endpoint-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_bytes_serializer(data):\n",
    "    f = io.BytesIO()\n",
    "    np.save(f, data)\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "optimized_predictor.content_type = 'application/vnd+python.numpy+binary'\n",
    "optimized_predictor.serializer = numpy_bytes_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the endpoints to get average latency stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original model on M4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 8\n",
      "prediction is 8\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 8\n",
      "prediction is 8\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "CPU times: user 750 ms, sys: 241 ms, total: 990 ms\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "for i in range(100):\n",
    "    data = mnist.test.images[i].tolist()\n",
    "    tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[1, len(data)], dtype=tf.float32)\n",
    "    predict_response = mnist_predictor.predict(tensor_proto)\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = predict_response['outputs']['classes']['int64_val'][0]\n",
    "    print(\"prediction is {}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiled M4 model on M4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 8\n",
      "prediction is 8\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 8\n",
      "prediction is 8\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 3\n",
      "prediction is 3\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 6\n",
      "prediction is 6\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "CPU times: user 853 ms, sys: 130 ms, total: 983 ms\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from IPython import display\n",
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "for i in range(100):\n",
    "    data = mnist.test.images[i]\n",
    "    # Display imageM\n",
    "    #im = PIL.Image.fromarray(data.reshape((28,28))*255).convert('L')\n",
    "    #display.display(im)\n",
    "    # Invoke endpoint with image\n",
    "    predict_response = optimized_predictor.predict(data)\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = np.argmax(predict_response)\n",
    "    print(\"prediction is {}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go to Cloudwatch to view invocation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#metricsV2:graph=~(view~'timeSeries~stacked~false~metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'original-mnist-model-endpoint-v1~'VariantName~'AllTraffic)~(~'...~'compiled-m4-mnist-model-endpoint-v1~'.~'.))~region~'us-east-1~start~'-PT15M~end~'P0D~stat~'Maximum~period~1);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d\n"
     ]
    }
   ],
   "source": [
    "url = \"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#metricsV2:graph=~(view~'timeSeries~stacked~false~metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'{}~'VariantName~'AllTraffic)~(~'...~'{}~'.~'.))~region~'us-east-1~start~'-PT15M~end~'P0D~stat~'Maximum~period~1);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d\"\n",
    "url = url.format(mnist_predictor.endpoint,optimized_predictor.endpoint)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Click ^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(optimized_predictor.endpoint)\n",
    "sagemaker.Session().delete_endpoint(mnist_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFlite (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://sagemaker-us-east-1-497456752804/neo-compile-test-2019-10-17-23-21-27-014/output/model.tar.gz ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r compiled/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export/\n",
      "export/Servo/\n",
      "export/Servo/1571354731/\n",
      "export/Servo/1571354731/variables/\n",
      "export/Servo/1571354731/variables/variables.data-00000-of-00001\n",
      "export/Servo/1571354731/variables/variables.index\n",
      "export/Servo/1571354731/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "!mkdir compiled & tar -xvzf model.tar.gz --directory compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13101152"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('./compiled/export/Servo/1571354731/')\n",
    "tflite_model = converter.convert()\n",
    "open(\"converted_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\r\n",
      "drwxr-xr-x 3 ec2-user ec2-user 4.0K Oct 17 23:25 Servo\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh compiled/export/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ec2-user ec2-user 13M Oct 18 16:10 converted_model.tflite\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh conv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
